# AWS

Notes about the very basics of AWS for a developer.

## Why

Vertical scalability vs Horizontal scalability.
Amazon automatises horizontal scalability and provisioning for its employees through an api, then it becomes AWS as IAAS (Infrastructure as a service).

## Regions

We plan what region should held our services according to our expected scenario.
Regions are isolated. An example could be "Europe (Paris) eu-west-3".

## Availability zones

Each region can have one or more availability zones.  
An availability zone is physical datacenter.

## VPC

As in a traditional datacenter, servers running in the cloud live inside a network, Amazon names this is Virtual private cloud.  
Inside a region we configure one or more VPCs, a small private network in the AWS cloud.  

Each VPC has assigned a range of IPs.  
Each VPC could have subnets (subsets of IPs) that we can assign to a certain availability zone.  
In that manner, if one datacenter (availability zone) is down, our application can still work running in another availability zone.  

We can use subnets to protect certain services from be accessed from the outside. Having public and private subnets.  

Also, it is recommended to create a VPC per environment in the context of web development. For instance, we could have 3 VPC: one for QA, one for Pre production and one for Production. VPCs are isolated so there are no risks of those communicating between them.  

The communication between services in a VPC is much faster than the communication throughout internet. By default, our services are running in a VPC.  

## AMI

Amazon machine image.  
Basically is the OS that my EC2 instance will run.  
The AWS marketplace can help to choose the right one.  
We can also create our own images of instances already created. It is a paying option per ami per region.

## EC2

We can choose the type of instance.  
Each type has a certain amount of memory and cpu, but we cannot choose a cpu and memory directly.  
Some types are more suitable for certain uses. C instances has more cpu power while R instances can have more memory or M instances are adapted to general purpose apps and so on.  
Small, large, extra large are concepts that help to choose an instances among the instances of the same family.  

### Key Pairs

We need a key pair to log into our instance. AWS supports RSA keys.

I can use my own public key or create a new one in AWS. If I import my own key, AWS will put this key into each instance I create granting me access to the instance via ssh.

### Security groups

Kind of firewall to define which servers can connect to ours. Allow or disallow traffic from or to our instances.  
It could recall iptables but one important difference is that all the rules in security groups are rules to allow something. By default, all kind of traffic is disallowed.  
Security groups are associated to a VPC.

The source of inbound traffic can be referred using another security group.

For instance, if I want to connect to my instance via ssh, I need to create a security group to allow TCP on port 22 from a given source. Some presets exist to autocomplete the record.

### New instance

With all that, we can create an instance.  

The whole process is pretty straightforward.   
We pick an OS. 
We have to choose security groups, vpc etc previously configured.  
We can add storage.  
The option "user data" allows us to paste a bash script that will be executed once the first time the instance is created.  

Instances are somehow volatile, so we should store important files in another places like CloudWatch (logs) or S3 (static files).  

Then you can connect for example via ssh:
`ssh -i /path/private/key user@ip`

The IP of the instance can change if you stop the instance, and then you launch it. If it is a dependence somewhere, remember to change it.

We can consult the logs of the instance initiation:
```
tail -f /var/log/cloud-init-output.log
```

## ROUTE 53

DNS and domain management service.  

If your domain is not registered by Amazon, first thing you can do is creating the NS records to map your domain and the nameservers of Amazon. Then you can create another records for your domain, such as an email record or a new subdomain etc.  

We usually create A records (map to an IP) and CNAME records (redirect to another service to resolve the name).  

For instance, we can create an A record for a subdomain `test.example.org` pointing to the IP of the EC2 instance.  

We can use the tool dig to check everything is ok:
```
# see if the IP match the one in the DNS record
dig test.example.org
# check a particular type of record
dig CNAME example.org
# check only the IP
dig +short test.example.org
# check the trace
dig +trace example.org
```

## CUSTOM AMI

Instead of using the AMIs (Amazon machine images) of Amazon, we can create our own AMI containing all the stuff we need.
We can pick an existing instance created previously. In the context menu, click on "create an image".   
Then it will be available when creating a new instance.  

We may create an AMI of a running instance in a given moment (a particular commit).

This image will be a copy, and it will have the same OS, packages, code etc already present in the original image.

## LOAD BALANCING

Distribute requests among many servers efficiently.

### Via DNS

It is possible to balance the load without AWS using DNS. The resolver can obtain more than one IP for a given name. The result is that in every TTL cycle my browser is going to use a different IP, distributing the load of the servers.  
The problem is that DNS is cached, if one of the instances changes its IP, until the TTL is finished, the user will send request to an IP no longer available.  
Also, if I create a new instance, it will not receive traffic until the TTL is done.

### Via Load balancer

All the requests are treated by the load balancer. In that scenario it will have a strategy to balance.  
The problem of the DNS cache does not apply in that case. The load balancer knows when a new instance is created.  

### Internal or Internet-facing

Two schemes: Internal or Internet-facing.

Internal load balancers resolve to internal IPs of the VPC, only accessible in the VPC. Amazon will asign an internal IP to this Load balancer.
Internet-facing will resolve to public IPs accessible from anywhere. Amazon will assign a public IP to this Load balancer.

### Security group

We need to create a new security group for our load balancer. For instance, allow http traffic from anywhere.  
Instances in the VPC have their owns security groups associated. If we want this kind of traffic reach the instances behind the load balancer, we may need to configure the security groups of the instances as well.  
To do that, we can add a new rule for a given security group, but this time the source is not a IP, but another security group, the one of the load balancer.

### Types

#### Network

It your applications are not http based, as logstash you can use this kind of load balancer.

#### Application

It knows http protocol. We can use http protocol concepts to balance the load.

Listener

We chose the listener during the configuration process: In which port the balancer is listening. If we chose https we need a TLS certificate. Amazon can provide it through ACM.

Availability zones

Also, we choose the availability zones (datacenters) where the instances are handling the requests of the load balancer.

Target group

The more important step might be the Routing. We create a target group (group of instances receiving the traffic).  
A target group usually contains a number or EC2 instances. All the instances keep the same application.  
Each application has its own life cycle, so it is better to configure one target group per application.  

We can configure several targets groups behind one ELB. In that manner we can hide several applications (one app per target group) behind one load balancer. 
We can achieve that editing the rules of the listener of the load balancer and map another host or path to a different target group.

Health check

The load balancer checks via http the health of each instance, this way the balancer will not send requests if the health check is not successful.

Auto scaling group

Instead of manually creating new instances and associate them to one particular target group we can automatically handle this with auto scaling groups.   
If one instance stop working the auto scaling group will automatically create another one and replace the one failing. We can be more resilient.

The auto scaling group needs to know how to create one instance, so we need a "launch configuration".

For instance, we could have one launch configuration per AMI and version (Myapp name v.1.1.1)
Then we configure an auto scaling group to use this launch configuration for a given target group.
Automatically the instances of the autoscaling group will be initiated so that the load balancer can send requests.

The autoscaling group can have a fix number of instances, or we can define rules to auto scale up or down with scaling policies.
